{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from numpy.random import randint\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn import discriminant_analysis\n",
    "\n",
    "from my_functions import make_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jackie\\Documents\\GitHub\\Data_Proj\n"
     ]
    }
   ],
   "source": [
    "cd ~/Documents/GitHub/Data_Proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gene_corr_base = pd.read_csv('gene_corr_base.csv', index_col = None, header=None)\n",
    "gene_corr_cut = pd.read_csv('gene_corr_cut.csv', index_col = None, header=None)\n",
    "gene_corr_p = pd.read_csv('gene_corr_p.csv', index_col = None, header=None)\n",
    "gene_names = pd.read_csv('gene_names.csv', index_col = None, header=None)\n",
    "df_full = pd.read_csv('post_imputation.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CDRSB</th>\n",
       "      <th>ADAS11</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>FAQ</th>\n",
       "      <th>MOCA</th>\n",
       "      <th>EcogSPMem</th>\n",
       "      <th>EcogSPLang</th>\n",
       "      <th>EcogSPVisspat</th>\n",
       "      <th>EcogSPPlan</th>\n",
       "      <th>...</th>\n",
       "      <th>M</th>\n",
       "      <th>update_stamp</th>\n",
       "      <th>First_DX</th>\n",
       "      <th>First_date</th>\n",
       "      <th>Final_DX</th>\n",
       "      <th>Final_date</th>\n",
       "      <th>First_Delta_Time</th>\n",
       "      <th>Final_Delta_Time</th>\n",
       "      <th>DX_Final_Progression</th>\n",
       "      <th>DX_Final_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>2017-10-06 23:19:46.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-06-16</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-06-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.250</td>\n",
       "      <td>3.66667</td>\n",
       "      <td>1.28571</td>\n",
       "      <td>2.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-06 23:19:54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-08-25</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-10-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.11111</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-06 23:19:55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-09-13</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-09-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1472.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-06 23:19:55.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-09-27</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-10-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-06 23:19:55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-10-04</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1477.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49435 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CDRSB  ADAS11  ADAS13  MMSE  FAQ  MOCA  EcogSPMem  EcogSPLang  \\\n",
       "7     0.0     3.0     4.0  30.0  1.0  28.0      1.125     1.00000   \n",
       "14    1.5    16.0    24.0  28.0  5.0  20.0      3.250     3.66667   \n",
       "22    0.0     7.0     8.0  28.0  0.0  21.0      1.000     1.11111   \n",
       "27    1.0     7.0     9.0  30.0  0.0  22.0      1.000     1.00000   \n",
       "35    0.0     6.0    11.0  28.0  0.0  25.0      1.250     1.00000   \n",
       "\n",
       "    EcogSPVisspat  EcogSPPlan      ...         M           update_stamp  \\\n",
       "7         1.00000        1.00      ...        60  2017-10-06 23:19:46.0   \n",
       "14        1.28571        2.25      ...         0  2017-10-06 23:19:54.0   \n",
       "22        1.00000        1.00      ...         0  2017-10-06 23:19:55.0   \n",
       "27        1.00000        1.00      ...         0  2017-10-06 23:19:55.0   \n",
       "35        1.00000        1.00      ...         0  2017-10-06 23:19:55.0   \n",
       "\n",
       "    First_DX  First_date  Final_DX  Final_date  First_Delta_Time  \\\n",
       "7          0  2011-06-16         0  2015-06-02               0.0   \n",
       "14         1  2011-08-25         2  2013-10-29               0.0   \n",
       "22         0  2011-09-13         0  2015-09-24               0.0   \n",
       "27         1  2011-09-27         1  2012-10-02               0.0   \n",
       "35         0  2011-10-04         0  2015-10-20               0.0   \n",
       "\n",
       "    Final_Delta_Time  DX_Final_Progression  DX_Final_Rate  \n",
       "7             1447.0                   0.0       0.000000  \n",
       "14             796.0                   1.0       0.001256  \n",
       "22            1472.0                   0.0       0.000000  \n",
       "27             371.0                   0.0       0.000000  \n",
       "35            1477.0                   0.0       0.000000  \n",
       "\n",
       "[5 rows x 49435 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>11715100_at</th>\n",
       "      <th>11715101_s_at</th>\n",
       "      <th>11715102_x_at</th>\n",
       "      <th>11715103_x_at</th>\n",
       "      <th>11715104_s_at</th>\n",
       "      <th>11715105_at</th>\n",
       "      <th>11715106_x_at</th>\n",
       "      <th>11715107_s_at</th>\n",
       "      <th>11715108_x_at</th>\n",
       "      <th>11715109_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX-r2-TagH_at</th>\n",
       "      <th>AFFX-r2-TagIN-3_at</th>\n",
       "      <th>AFFX-r2-TagIN-5_at</th>\n",
       "      <th>AFFX-r2-TagIN-M_at</th>\n",
       "      <th>AFFX-r2-TagJ-3_at</th>\n",
       "      <th>AFFX-r2-TagJ-5_at</th>\n",
       "      <th>AFFX-r2-TagO-3_at</th>\n",
       "      <th>AFFX-r2-TagO-5_at</th>\n",
       "      <th>AFFX-r2-TagQ-3_at</th>\n",
       "      <th>AFFX-r2-TagQ-5_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.004618</td>\n",
       "      <td>0.696194</td>\n",
       "      <td>-1.317020</td>\n",
       "      <td>0.315948</td>\n",
       "      <td>-0.034504</td>\n",
       "      <td>-0.378302</td>\n",
       "      <td>0.612318</td>\n",
       "      <td>-0.203451</td>\n",
       "      <td>-0.206527</td>\n",
       "      <td>-0.301305</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.334344</td>\n",
       "      <td>-0.096450</td>\n",
       "      <td>-0.054353</td>\n",
       "      <td>0.639972</td>\n",
       "      <td>-1.911019</td>\n",
       "      <td>-0.309482</td>\n",
       "      <td>-1.040793</td>\n",
       "      <td>-0.536355</td>\n",
       "      <td>-1.040062</td>\n",
       "      <td>-0.749751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.200963</td>\n",
       "      <td>-0.476742</td>\n",
       "      <td>0.331783</td>\n",
       "      <td>1.228517</td>\n",
       "      <td>-0.655580</td>\n",
       "      <td>-0.343305</td>\n",
       "      <td>0.378144</td>\n",
       "      <td>1.305417</td>\n",
       "      <td>-1.739029</td>\n",
       "      <td>1.347586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154246</td>\n",
       "      <td>0.594417</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.032531</td>\n",
       "      <td>-0.757878</td>\n",
       "      <td>-1.838261</td>\n",
       "      <td>-0.209825</td>\n",
       "      <td>-0.108993</td>\n",
       "      <td>0.539111</td>\n",
       "      <td>0.087313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.419879</td>\n",
       "      <td>-0.319452</td>\n",
       "      <td>-0.338550</td>\n",
       "      <td>0.713567</td>\n",
       "      <td>-0.544255</td>\n",
       "      <td>0.125656</td>\n",
       "      <td>-0.095186</td>\n",
       "      <td>0.399328</td>\n",
       "      <td>-1.401597</td>\n",
       "      <td>1.846088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.381326</td>\n",
       "      <td>-0.711159</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.657837</td>\n",
       "      <td>-0.397119</td>\n",
       "      <td>1.068100</td>\n",
       "      <td>-0.025166</td>\n",
       "      <td>-0.240972</td>\n",
       "      <td>-0.780795</td>\n",
       "      <td>0.287931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.563242</td>\n",
       "      <td>0.210841</td>\n",
       "      <td>-0.192590</td>\n",
       "      <td>0.782010</td>\n",
       "      <td>-1.018851</td>\n",
       "      <td>-0.511291</td>\n",
       "      <td>-1.609842</td>\n",
       "      <td>0.215039</td>\n",
       "      <td>1.818063</td>\n",
       "      <td>-0.509470</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.608406</td>\n",
       "      <td>-1.314987</td>\n",
       "      <td>0.509290</td>\n",
       "      <td>0.693569</td>\n",
       "      <td>0.105368</td>\n",
       "      <td>0.396109</td>\n",
       "      <td>-0.080563</td>\n",
       "      <td>-1.749311</td>\n",
       "      <td>0.931940</td>\n",
       "      <td>-1.510718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-1.922725</td>\n",
       "      <td>-1.231736</td>\n",
       "      <td>-1.333238</td>\n",
       "      <td>-1.043128</td>\n",
       "      <td>-0.456367</td>\n",
       "      <td>0.650612</td>\n",
       "      <td>-0.289500</td>\n",
       "      <td>-2.088576</td>\n",
       "      <td>0.074666</td>\n",
       "      <td>-1.265440</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.905960</td>\n",
       "      <td>-0.787317</td>\n",
       "      <td>-1.389296</td>\n",
       "      <td>-0.414116</td>\n",
       "      <td>-0.197413</td>\n",
       "      <td>0.152512</td>\n",
       "      <td>-0.966929</td>\n",
       "      <td>0.645177</td>\n",
       "      <td>0.067716</td>\n",
       "      <td>-0.888108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    11715100_at  11715101_s_at  11715102_x_at  11715103_x_at  11715104_s_at  \\\n",
       "7     -0.004618       0.696194      -1.317020       0.315948      -0.034504   \n",
       "14    -1.200963      -0.476742       0.331783       1.228517      -0.655580   \n",
       "22    -0.419879      -0.319452      -0.338550       0.713567      -0.544255   \n",
       "27    -0.563242       0.210841      -0.192590       0.782010      -1.018851   \n",
       "35    -1.922725      -1.231736      -1.333238      -1.043128      -0.456367   \n",
       "\n",
       "    11715105_at  11715106_x_at  11715107_s_at  11715108_x_at  11715109_at  \\\n",
       "7     -0.378302       0.612318      -0.203451      -0.206527    -0.301305   \n",
       "14    -0.343305       0.378144       1.305417      -1.739029     1.347586   \n",
       "22     0.125656      -0.095186       0.399328      -1.401597     1.846088   \n",
       "27    -0.511291      -1.609842       0.215039       1.818063    -0.509470   \n",
       "35     0.650612      -0.289500      -2.088576       0.074666    -1.265440   \n",
       "\n",
       "          ...          AFFX-r2-TagH_at  AFFX-r2-TagIN-3_at  \\\n",
       "7         ...                -0.334344           -0.096450   \n",
       "14        ...                -0.154246            0.594417   \n",
       "22        ...                -0.381326           -0.711159   \n",
       "27        ...                -0.608406           -1.314987   \n",
       "35        ...                -0.905960           -0.787317   \n",
       "\n",
       "    AFFX-r2-TagIN-5_at  AFFX-r2-TagIN-M_at  AFFX-r2-TagJ-3_at  \\\n",
       "7            -0.054353            0.639972          -1.911019   \n",
       "14            0.998769            0.032531          -0.757878   \n",
       "22           -0.454836            0.657837          -0.397119   \n",
       "27            0.509290            0.693569           0.105368   \n",
       "35           -1.389296           -0.414116          -0.197413   \n",
       "\n",
       "    AFFX-r2-TagJ-5_at  AFFX-r2-TagO-3_at  AFFX-r2-TagO-5_at  \\\n",
       "7           -0.309482          -1.040793          -0.536355   \n",
       "14          -1.838261          -0.209825          -0.108993   \n",
       "22           1.068100          -0.025166          -0.240972   \n",
       "27           0.396109          -0.080563          -1.749311   \n",
       "35           0.152512          -0.966929           0.645177   \n",
       "\n",
       "    AFFX-r2-TagQ-3_at  AFFX-r2-TagQ-5_at  \n",
       "7           -1.040062          -0.749751  \n",
       "14           0.539111           0.087313  \n",
       "22          -0.780795           0.287931  \n",
       "27           0.931940          -1.510718  \n",
       "35           0.067716          -0.888108  \n",
       "\n",
       "[5 rows x 49386 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just gene expression data\n",
    "X_full = df_full.iloc[:,14:49400]\n",
    "X_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene_Name</th>\n",
       "      <th>Base</th>\n",
       "      <th>Cut</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42374</th>\n",
       "      <td>11757474_x_at</td>\n",
       "      <td>0.234427</td>\n",
       "      <td>0.234427</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31151</th>\n",
       "      <td>11746251_x_at</td>\n",
       "      <td>0.212479</td>\n",
       "      <td>0.212479</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6909</th>\n",
       "      <td>11722009_a_at</td>\n",
       "      <td>0.198839</td>\n",
       "      <td>0.198839</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47835</th>\n",
       "      <td>11762935_x_at</td>\n",
       "      <td>0.187445</td>\n",
       "      <td>0.187445</td>\n",
       "      <td>0.000357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15665</th>\n",
       "      <td>11730765_at</td>\n",
       "      <td>0.174991</td>\n",
       "      <td>0.174991</td>\n",
       "      <td>0.000755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10789</th>\n",
       "      <td>11725889_at</td>\n",
       "      <td>0.166186</td>\n",
       "      <td>0.166186</td>\n",
       "      <td>0.000824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7203</th>\n",
       "      <td>11722303_x_at</td>\n",
       "      <td>0.169622</td>\n",
       "      <td>0.169622</td>\n",
       "      <td>0.000936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4518</th>\n",
       "      <td>11719618_a_at</td>\n",
       "      <td>0.161189</td>\n",
       "      <td>0.161189</td>\n",
       "      <td>0.001258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26090</th>\n",
       "      <td>11741190_a_at</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.001670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6810</th>\n",
       "      <td>11721910_at</td>\n",
       "      <td>0.160999</td>\n",
       "      <td>0.160999</td>\n",
       "      <td>0.001846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32506</th>\n",
       "      <td>11747606_a_at</td>\n",
       "      <td>0.151940</td>\n",
       "      <td>0.151940</td>\n",
       "      <td>0.001913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46475</th>\n",
       "      <td>11761575_at</td>\n",
       "      <td>0.155691</td>\n",
       "      <td>0.155691</td>\n",
       "      <td>0.002033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9183</th>\n",
       "      <td>11724283_a_at</td>\n",
       "      <td>0.162228</td>\n",
       "      <td>0.162228</td>\n",
       "      <td>0.002094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10788</th>\n",
       "      <td>11725888_at</td>\n",
       "      <td>0.154584</td>\n",
       "      <td>0.154584</td>\n",
       "      <td>0.002121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18119</th>\n",
       "      <td>11733219_x_at</td>\n",
       "      <td>0.153854</td>\n",
       "      <td>0.153854</td>\n",
       "      <td>0.002261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32487</th>\n",
       "      <td>11747587_a_at</td>\n",
       "      <td>0.153094</td>\n",
       "      <td>0.153094</td>\n",
       "      <td>0.002452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26628</th>\n",
       "      <td>11741728_x_at</td>\n",
       "      <td>0.152116</td>\n",
       "      <td>0.152116</td>\n",
       "      <td>0.002461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12501</th>\n",
       "      <td>11727601_a_at</td>\n",
       "      <td>0.155935</td>\n",
       "      <td>0.155935</td>\n",
       "      <td>0.002570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43104</th>\n",
       "      <td>11758204_s_at</td>\n",
       "      <td>0.156271</td>\n",
       "      <td>0.156271</td>\n",
       "      <td>0.002714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22057</th>\n",
       "      <td>11737157_x_at</td>\n",
       "      <td>0.150977</td>\n",
       "      <td>0.150977</td>\n",
       "      <td>0.002723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Gene_Name      Base       Cut         p\n",
       "42374  11757474_x_at  0.234427  0.234427  0.000019\n",
       "31151  11746251_x_at  0.212479  0.212479  0.000037\n",
       "6909   11722009_a_at  0.198839  0.198839  0.000216\n",
       "47835  11762935_x_at  0.187445  0.187445  0.000357\n",
       "15665    11730765_at  0.174991  0.174991  0.000755\n",
       "10789    11725889_at  0.166186  0.166186  0.000824\n",
       "7203   11722303_x_at  0.169622  0.169622  0.000936\n",
       "4518   11719618_a_at  0.161189  0.161189  0.001258\n",
       "26090  11741190_a_at  0.160000  0.160000  0.001670\n",
       "6810     11721910_at  0.160999  0.160999  0.001846\n",
       "32506  11747606_a_at  0.151940  0.151940  0.001913\n",
       "46475    11761575_at  0.155691  0.155691  0.002033\n",
       "9183   11724283_a_at  0.162228  0.162228  0.002094\n",
       "10788    11725888_at  0.154584  0.154584  0.002121\n",
       "18119  11733219_x_at  0.153854  0.153854  0.002261\n",
       "32487  11747587_a_at  0.153094  0.153094  0.002452\n",
       "26628  11741728_x_at  0.152116  0.152116  0.002461\n",
       "12501  11727601_a_at  0.155935  0.155935  0.002570\n",
       "43104  11758204_s_at  0.156271  0.156271  0.002714\n",
       "22057  11737157_x_at  0.150977  0.150977  0.002723"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs_df = pd.DataFrame()\n",
    "corrs_df['Gene_Name'] = gene_names[0]\n",
    "corrs_df['Base'] = gene_corr_base[0]/15\n",
    "corrs_df['Cut'] = gene_corr_cut[0]/15\n",
    "corrs_df['p'] = gene_corr_p[0]/15\n",
    "corrs_df.sort_values('p').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Used = corrs_df[corrs_df.p < 0.01]\n",
    "used_genes = list(Used.Gene_Name.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def for_loop_status(length, index = 0):\n",
    "    sys.stdout.write('\\r%f%%' % ((index/length)*100))\n",
    "    sys.stdout.flush()\n",
    "    index += 1\n",
    "    return(index)\n",
    "\n",
    "def for_loop_status2(length, length2, index = 0, index2 = 0):\n",
    "    sys.stdout.write('\\r%f%% of %f%%' % ((index/length)*100, (index2/length2)*100))\n",
    "    sys.stdout.flush()\n",
    "    index += 1\n",
    "    index2 += 1\n",
    "    return(index, index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.481865%"
     ]
    }
   ],
   "source": [
    "# interaction and polynomial terms\n",
    "predictors_expo_list = []\n",
    "for_index = 0\n",
    "index = 0\n",
    "for predictor in used_genes:\n",
    "    for_index = for_loop_status(len(used_genes), for_index)\n",
    "    predictors_temp_list = []\n",
    "    for expo in [2,3,4]:\n",
    "        #if expo = 2 and the predictor is atemp, then you'll get \"atemp2\" which is atemp^2\n",
    "        predictors_temp_list.append('{0}{1}'.format(predictor, expo))\n",
    "        df_full['{0}{1}'.format(predictor, expo)] = df_full['{0}'.format(predictor)]**int('{0}'.format(expo))\n",
    "    predictors_expo_list.append(predictors_temp_list)\n",
    "    for predictor2 in used_genes[int(index + 1):]:\n",
    "        if predictor != predictor2:\n",
    "            df_full['{0}_x_{1}'.format(predictor, predictor2)] = df_full['{0}'.format(predictor)] * df_full['{0}'.format(predictor2)]\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['11715176_at2', '11715176_at3', '11715176_at4',\n",
       "       '11715176_at_x_11715574_a_at', '11715176_at_x_11715665_a_at',\n",
       "       '11715176_at_x_11716095_s_at', '11715176_at_x_11716149_a_at',\n",
       "       '11715176_at_x_11716279_x_at', '11715176_at_x_11716702_a_at',\n",
       "       '11715176_at_x_11717003_s_at',\n",
       "       ...\n",
       "       '11762935_x_at4', '11762935_x_at_x_11762998_x_at',\n",
       "       '11762935_x_at_x_200064_PM_at', '11762998_x_at2', '11762998_x_at3',\n",
       "       '11762998_x_at4', '11762998_x_at_x_200064_PM_at', '200064_PM_at2',\n",
       "       '200064_PM_at3', '200064_PM_at4'],\n",
       "      dtype='object', length=19107)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find index of first polynomial term\n",
    "firstpoly = df_full.columns.get_loc('DX_Final_Rate') + 1\n",
    "df_full.columns[firstpoly:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_full_removed = df_full[df_full['DX_Final_Progression'] != 0]\n",
    "df_full = df_full[df_full['DX_Final_Progression'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_genes = used_genes.copy()\n",
    "#significant genes and new poly/interaction terms\n",
    "full_genes = temp_genes + list(df_full.columns[firstpoly:].values)\n",
    "X_full = df_full[full_genes]\n",
    "y_full = df_full['DX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "np.random.seed(9001)\n",
    "msk = np.random.rand(len(X_full)) < 0.5\n",
    "\n",
    "X_train = X_full[msk]\n",
    "X_test = X_full[~msk]\n",
    "y_train = y_full[msk]\n",
    "y_test = y_full[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jackie\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Jackie\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "pre_X_train = X_full[msk]\n",
    "pre_X_test = X_full[~msk]\n",
    "pre_y_train = y_full[msk]\n",
    "pre_y_test = y_full[~msk]\n",
    "\n",
    "pre_X_train['class'] = pre_y_train\n",
    "pre_X_test['class'] = pre_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PCA with all components\n",
    "pca_full_fit = PCA()\n",
    "pca_full = {}\n",
    "pca_full['Xtrain'] = pca_full_fit.fit_transform(pre_X_train)\n",
    "\n",
    "# find number of components that explain 90% of predictor variance\n",
    "n_components = (np.argwhere((np.cumsum(pca_full_fit.explained_variance_ratio_)) > 0.9)[0] + 1)[0]\n",
    "\n",
    "# PCA with n_components\n",
    "pca90 = {}\n",
    "pca90_fit = PCA(n_components)\n",
    "pca90_fit.fit(pre_X_train)\n",
    "X_train = pca90_fit.transform(pre_X_train)\n",
    "X_test = pca90_fit.transform(pre_X_test)\n",
    "y_train = pre_y_train\n",
    "y_test = pre_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random classifier\n",
    "rand = {}\n",
    "rand['train_pred'] = randint(3, size = len(X_train))\n",
    "rand['test_pred'] = randint(3, size = len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost with PCA and CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=0.60000000000000009, n_estimators=19,\n",
       "          random_state=9001)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_dict = OrderedDict(\n",
    "    n_estimators = range(1,20),\n",
    "    learning_rate = np.arange(0.05,1,0.05)\n",
    ")\n",
    "\n",
    "est = AdaBoostClassifier(random_state = 9001)\n",
    "gb_cv = GridSearchCV(est, param_grid = param_dict, cv=3, n_jobs=-1)\n",
    "gb_cv.fit(X_train, y_train)\n",
    "gb_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost with PCA & CV, train:  0.664705882353\n",
      "AdaBoost with PCA & CV, test:  0.421052631579\n",
      "Random class. accuracy, train:  0.264705882353\n",
      "Random class. accuracy, test:  0.310526315789\n"
     ]
    }
   ],
   "source": [
    "print('AdaBoost with PCA & CV, train: ', accuracy_score(y_train, gb_cv.best_estimator_.fit(X_train, y_train).predict(X_train)))\n",
    "print('AdaBoost with PCA & CV, test: ', accuracy_score(y_test, gb_cv.best_estimator_.fit(X_train, y_train).predict(X_test)))\n",
    "print('Random class. accuracy, train: ', accuracy_score(y_train, rand['train_pred']))\n",
    "print('Random class. accuracy, test: ', accuracy_score(y_test, rand['test_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression, Discriminant Analysis, kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jackie\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:695: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "# fit logistic regression model\n",
    "lin_class = {}\n",
    "lin_class['logit_ovr'] = LogisticRegressionCV(Cs=7, penalty='l2', random_state = 9001, multi_class='ovr')\n",
    "lin_class['logit_ovr'].fit(X_train, y_train)\n",
    "\n",
    "# classify\n",
    "lin_class['logit_ovr_train'] = lin_class['logit_ovr'].predict(X_train)\n",
    "lin_class['logit_ovr_test'] = lin_class['logit_ovr'].predict(X_test)\n",
    "\n",
    "# fit logistic regression model\n",
    "lin_class['logit_multinomial'] = LogisticRegressionCV(Cs=7, penalty='l2', random_state = 9001, multi_class='multinomial')\n",
    "lin_class['logit_multinomial'].fit(X_train, y_train)\n",
    "\n",
    "# classify\n",
    "lin_class['logit_multinomial_train'] = lin_class['logit_multinomial'].predict(X_train)\n",
    "lin_class['logit_multinomial_test'] = lin_class['logit_multinomial'].predict(X_test)\n",
    "\n",
    "# LDA\n",
    "lda = {}\n",
    "lda['model'] = discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "lda['model'].fit(X_train, y_train)\n",
    "lda['train_pred'] = lda['model'].predict(X_train)\n",
    "lda['test_pred'] = lda['model'].predict(X_test)\n",
    "\n",
    "# QDA\n",
    "qda = {}\n",
    "qda['model'] = discriminant_analysis.QuadraticDiscriminantAnalysis()\n",
    "qda['model'].fit(X_train, y_train)\n",
    "qda['train_pred'] = qda['model'].predict(X_train)\n",
    "qda['test_pred'] = qda['model'].predict(X_test)\n",
    "\n",
    "#Fit a knn model\n",
    "knn = {}\n",
    "\n",
    "# knn cv code from https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/#parameter-tuning-with-cross-validation\n",
    "knn['cv_scores'] = []\n",
    "knn['neighbours'] = list(range(1,50))\n",
    "for k in knn['neighbours']:\n",
    "    current_model = KNN(n_neighbors=k)\n",
    "    scores = cross_val_score(current_model, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    knn['cv_scores'].append(scores.mean())\n",
    "knn['MSE'] = [1 - x for x in knn['cv_scores']]\n",
    "# determining best k\n",
    "knn['optimal_k'] = knn['neighbours'][knn['MSE'].index(min(knn['MSE']))]\n",
    "\n",
    "knn['model'] = KNN(n_neighbors=knn['optimal_k'])\n",
    "knn['model'].fit(X_train, y_train)\n",
    "knn['train_pred'] = knn['model'].predict(X_train)\n",
    "knn['test_pred'] = knn['model'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic OvR class. accuracy, train:  0.635294117647\n",
      "Logistic OvR class. accuracy, test:  0.468421052632\n",
      "Logistic multinomial class. accuracy, train:  0.770588235294\n",
      "Logistic multinomial class. accuracy, test:  0.347368421053\n",
      "LDA class. accuracy, train:  0.611764705882\n",
      "LDA class. accuracy, test:  0.473684210526\n",
      "QDA class. accuracy, train:  0.888235294118\n",
      "QDA class. accuracy, test:  0.426315789474\n",
      "KNN class. accuracy, train:  0.723529411765\n",
      "KNN class. accuracy, test:  0.421052631579\n",
      "Random class. accuracy, train:  0.264705882353\n",
      "Random class. accuracy, test:  0.310526315789\n"
     ]
    }
   ],
   "source": [
    "# print all classification accuracies\n",
    "print('Logistic OvR class. accuracy, train: ', accuracy_score(y_train, lin_class['logit_ovr_train']))\n",
    "print('Logistic OvR class. accuracy, test: ', accuracy_score(y_test, lin_class['logit_ovr_test']))\n",
    "print('Logistic multinomial class. accuracy, train: ', accuracy_score(y_train, lin_class['logit_multinomial_train']))\n",
    "print('Logistic multinomial class. accuracy, test: ', accuracy_score(y_test, lin_class['logit_multinomial_test']))\n",
    "print('LDA class. accuracy, train: ', accuracy_score(y_train, lda['train_pred']))\n",
    "print('LDA class. accuracy, test: ', accuracy_score(y_test, lda['test_pred']))\n",
    "print('QDA class. accuracy, train: ', accuracy_score(y_train, qda['train_pred']))\n",
    "print('QDA class. accuracy, test: ', accuracy_score(y_test, qda['test_pred']))\n",
    "print('KNN class. accuracy, train: ', accuracy_score(y_train, knn['train_pred']))\n",
    "print('KNN class. accuracy, test: ', accuracy_score(y_test, knn['test_pred']))\n",
    "print('Random class. accuracy, train: ', accuracy_score(y_train, rand['train_pred']))\n",
    "print('Random class. accuracy, test: ', accuracy_score(y_test, rand['test_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree CV for max depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Xtrain.csv')\n",
    "df_test = pd.read_csv('Xtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_names = df_train.columns.values\n",
    "for value in orig_names:\n",
    "    if value[0] == ' ':\n",
    "        df_train = df_train.rename(columns = {value: value[1:]})\n",
    "        df_test = df_test.rename(columns = {value: value[1:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract matrices from dataframe\n",
    "dtrain_X_full = df_train.drop('class', axis = 1)\n",
    "#dtrain_X = df_train.iloc[:, :-8]\n",
    "dtest_X_full = df_test.drop('class', axis = 1)\n",
    "#dtest_X = df_test.iloc[:, :-8]\n",
    "\n",
    "dt_class = {}\n",
    "dt_class['Xtrain'] = dtrain_X_full.values\n",
    "dt_class['Xtest'] = dtest_X_full.values\n",
    "dt_class['ytrain'] = df_train['class'].values\n",
    "dt_class['ytest'] = df_test['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal tree depth is 9 with 0.347324 accuracy\n",
      "Tree class. accuracy, train:  1.0\n",
      "Tree class. accuracy, test:  0.363157894737\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFXCAYAAACLEMbVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25001ef7748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training and test accuracies as function of tree depth\n",
    "dt = {}\n",
    "depths = list(range(2,11))\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for i in depths:\n",
    "    dt[str(i)] = DecisionTreeClassifier(max_depth = i)\n",
    "    dt[str(i)].fit(dt_class['Xtrain'], dt_class['ytrain'])\n",
    "    train_scores.append(dt[str(i)].score(dt_class['Xtrain'], dt_class['ytrain']))\n",
    "    test_scores.append(dt[str(i)].score(dt_class['Xtest'], dt_class['ytest']))\n",
    "plt.plot(depths, train_scores, \"o-\", label = \"Training\")\n",
    "plt.plot(depths, test_scores, \"o-\", label = \"Test\")\n",
    "plt.xlabel(\"Tree depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# use 5-fold cross validation to find optimal tree depth\n",
    "cv_scores = []\n",
    "for i in depths:\n",
    "    cv_scores.append(np.mean(cross_val_score(dt[str(i)], dt_class[\"Xtrain\"], dt_class[\"ytrain\"], cv=5)))\n",
    "    cv_max = max(cv_scores)\n",
    "    opt_depth = depths[cv_scores.index(cv_max)]\n",
    "print(\"Optimal tree depth is %d with %f accuracy\" %(opt_depth, cv_max))\n",
    "max_depth = opt_depth\n",
    "\n",
    "# refit tree w/optimal tree depth\n",
    "cv_dt = DecisionTreeClassifier(max_depth = opt_depth)\n",
    "cv_dt.fit(dt_class[\"Xtrain\"], dt_class[\"ytrain\"])\n",
    "\n",
    "print('Tree class. accuracy, train: ', accuracy_score(dt_class[\"ytrain\"], cv_dt.predict(dt_class['Xtrain'])))\n",
    "print('Tree class. accuracy, test: ', accuracy_score(dt_class[\"ytest\"], cv_dt.predict(dt_class['Xtest'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
